{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias-variance trade-off\n",
    "\n",
    "There are three main contributers to error in a model:\n",
    " - Bias\n",
    " - Variance\n",
    " - Irreducible Error: Error contributed by dataset that cannot be removed unless dataset is changed\n",
    " \n",
    "There is a trade-off between bias and variance. Reducing model complexity usually increases bias and reduces variance, increasing model complexity is the opposite. What about ensemble models?\n",
    "\n",
    "Each individual predictor now has a higher bias then if it were trained on the whole dataset but when combining them together both the bias and variance are reduced.\n",
    "\n",
    "## Decision Trees classifier\n",
    "\n",
    "This classifier learns parameters to create a sequence of decision rules that classifies the data. We could come up with such trees ourselves but the decision tree classifier learns which parameters are most important. The greater the depth of the tree the greater the model complexity. This works well on the iris dataset and decision trees work well in general for classifying discrete data. Would not make much sense to apply it to continuous data.\n",
    "\n",
    "When applied to the dataset it can be seen that in the first level the easiest seperated data is seperated off and then the complexity of the boundary builds with the layers.\n",
    "\n",
    "It is most common to go from using a single decision tree to using a forest of decision trees which will give an irregular and jagged decision boundary. One has to decide to sample with replacement or not when it comes to random forests.\n",
    "\n",
    "## Random Forests Classifier\n",
    "\n",
    "- Allows you to control both how the trees are grown (using hyperparameters) and how the enemble is built\n",
    "- Extra randomness: instead of searching for the very best feature to split a node on, it searches for the best feature amongst a random subset of features.\n",
    "- Trading higher bias for lower variance -> overall more generalisable\n",
    "- Importance of each feature can be measured by looking at how much the nodes that using a particular feature reduce impurity on the average.\n",
    "- This can be used for quick understanding of which features matter (feature selection)\n",
    "- You can also sample randomly on the features as well as on the training instances.\n",
    "\n",
    "## Boosting\n",
    "Boosting stands for hypothesis boosting and it is an approach that can combine several weaker leaarners into a stronger learner. Most popular methods are AdaBoost and Gradient Boosting.\n",
    "\n",
    "AdaBoost: Start with first predictor, train and estimate performance, increase relative weight of misclassified training isntances, train new predictor on updated weights and continue sequentially until a certain cutoff. This hopefully creates a sequence of predictors which are convergent in error, however if the learning rate happens too fast you may overshoot and diverge.\n",
    "\n",
    "Gradient Boosting: The idea is to train predictors on the predeccessors residual errors.\n",
    "\n",
    " - Predict\n",
    " - Take residuals and create new predictor, sum with previous predictor\n",
    " \n",
    " The learning rate scales the contribution of each tree. The lower the rate, the more trees you'll need in the ensemble (but the predictors will usually generalise better).\n",
    " \n",
    "How do we know when to stop? Estimate the validation error and stop when it reaches a minimum or does not improve for a number of iterations.\n",
    "\n",
    "## Stacking\n",
    "\n",
    "Instead of using a trivial functiomn like hard voting to aggregate predictions, why not train a model to learn such aggregation? This model is called a blender or a meta-learner.\n",
    "You create a new layer in a NN that learns a predictor, or even a whole new NN itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
